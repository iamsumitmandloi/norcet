# NORCET Papers Repository

End-to-end pipeline for NORCET data preparation and query-ready delivery:

- Download and store year-wise PDFs
- Extract clean text from PDFs
- Parse MCQs while preserving original wording/options
- Tag by subject/topic/subtopic
- Build de-duplicated final JSON dataset
- Validate question integrity and year-wise counts
- Load into PostgreSQL
- Provide a frontend for year/subject/topic filtering and 2022 PDF actions

## Full folder structure

```text
norcet-papers/
├── README.md
├── requirements.txt
├── database/
│   └── schema.sql
├── extracted_text/
│   └── {year}.txt                   # generated by extract_pdf.py
├── frontend/
│   ├── index.html                   # year/subject/topic UI + PDF controls
│   ├── styles.css
│   └── app.js
├── logs/
│   ├── download_failures.log        # generated by downloader.py
│   ├── download_manifest.json       # generated by downloader.py
│   └── hash_manifest.json           # generated by downloader.py
├── raw_pdfs/
│   ├── 2012/ ... 2026/
│   └── unknown/
├── scripts/
│   ├── urls.txt
│   ├── downloader.py
│   ├── extract_pdf.py
│   ├── parse_mcq.py
│   ├── tag_questions.py
│   ├── build_dataset.py
│   ├── validate_dataset.py
│   └── load_to_postgres.py
└── structured_json/
    ├── sample_memory_mcqs.json
    ├── topic_keywords.json
    ├── tagged_questions.json
    ├── final_questions.json         # generated by build_dataset.py
    └── year_counts.json             # generated by build_dataset.py
```

## Install dependencies

```bash
python3 -m pip install -r norcet-papers/requirements.txt
```

## Scripts and what they do

### 1) Download PDFs

```bash
python3 norcet-papers/scripts/downloader.py --url-file norcet-papers/scripts/urls.txt
```

- Saves PDFs to `raw_pdfs/{year}/`
- Detects year from URL/headers/content
- Re-runnable and duplicate-safe using URL + SHA256 manifests

### 2) Extract clean text from PDFs

```bash
python3 norcet-papers/scripts/extract_pdf.py --root-dir norcet-papers
```

- Reads all PDFs in `raw_pdfs/*/*.pdf`
- Writes year-wise cleaned text in `extracted_text/{year}.txt`
- Removes repeated header/footer and common watermark noise

### 3) Parse MCQs from extracted text

```bash
python3 norcet-papers/scripts/parse_mcq.py --root-dir norcet-papers --year 2022
```

Optional metadata defaults:

```bash
python3 norcet-papers/scripts/parse_mcq.py \
  --root-dir norcet-papers \
  --year 2022 \
  --subject "Medical Surgical Nursing" \
  --topic "Cardiology" \
  --subtopic "Shock"
```

Output: `structured_json/{year}.json`

### 4) Tag subject/topic/subtopic

```bash
python3 norcet-papers/scripts/tag_questions.py --root-dir norcet-papers
```

Output: `structured_json/tagged_questions.json`

### 5) Build final de-duplicated dataset

```bash
python3 norcet-papers/scripts/build_dataset.py --root-dir norcet-papers
```

Outputs:

- `structured_json/final_questions.json`
- `structured_json/year_counts.json`

### 6) Validate data quality rules

```bash
python3 norcet-papers/scripts/validate_dataset.py --root-dir norcet-papers
```

Validation checks:

- No duplicate questions
- Options A/B/C/D present
- Correct answer integrity (`A|B|C|D`)
- Year-wise question count report

### 7) Load into PostgreSQL

```bash
python3 norcet-papers/scripts/load_to_postgres.py \
  --root-dir norcet-papers \
  --input structured_json/final_questions.json \
  --database-url "$DATABASE_URL"
```

- Uses upsert on `question_hash`
- Re-runnable for incremental updates
- Batch insert for scale (2000+ questions)

## Database schema

Apply schema:

```bash
psql "$DATABASE_URL" -f norcet-papers/database/schema.sql
```

Schema highlights:

- `question_hash` unique for dedupe/upsert
- normalized columns: year, subject, topic, subtopic
- option columns: `option_a` to `option_d`
- indexed for fast filtering

## Sample extracted JSON

From `structured_json/final_questions.json`:

```json
{
  "count": 5,
  "duplicates_removed": 5,
  "questions": [
    {
      "question_id": "norcet-2022-mem-001",
      "year": 2022,
      "subject": "Medical-Surgical Nursing",
      "topic": "IV Therapy",
      "subtopic": "Phlebitis",
      "question_text": "A nurse notes redness, tenderness and slowed IV flow at an IV cannula site. The correct interpretation and action are:",
      "options": {
        "A": "Normal finding, continue IV",
        "B": "Sepsis, stop IV and remove",
        "C": "Phlebitis, stop IV and remove",
        "D": "Abnormal - wait for doctor’s order"
      },
      "correct_answer": "C"
    }
  ]
}
```

## Frontend logic requirements mapping

Run with any static server from repo root, for example:

```bash
python3 -m http.server 8000
```

Open:

- `http://localhost:8000/norcet-papers/frontend/`

Implemented behaviors:

- Year selection filters questions
- If year is `2022`, shows:
  - `Start Test`
  - `View PDF`
  - `Download PDF`
  - embedded PDF panel (shown on `View PDF`)
- Subject/topic dropdowns filter questions further
- Every question card includes year tag: `Asked in NORCET <year>`

## Quality rules compliance

- No duplicate questions in final dataset (hash-based dedupe)
- Original wording preserved (`question_text` copied as parsed)
- Options preserved (order mapped A/B/C/D without rewriting text)
- Correct answer normalized and validated to option key
- Question count per year generated in `year_counts.json`

## Performance + modularity

- Pipeline scripts are independent and re-runnable
- Duplicate prevention at download and dataset build layers
- Batch DB loader for large datasets (2000+)
- Frontend renders efficiently using `DocumentFragment`

## End goal checklist

- ✔ NORCET PDFs can be stored under `raw_pdfs/{year}/`
- ✔ Clean extracted text via `extract_pdf.py`
- ✔ Structured MCQ JSON via `parse_mcq.py` + `build_dataset.py`
- ✔ Tagged/classified dataset via `tag_questions.py`
- ✔ Ready-to-query PostgreSQL database via `schema.sql` + `load_to_postgres.py`

## Study resource

- Curated previous year AIIMS nursing questions: `norcet-papers/aiims_nursing_exam_questions.md`
